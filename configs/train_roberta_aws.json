["llm_train.py",
"--save_steps", "10000",
"--logging_steps", "201",
"--eval_steps", "25",
"--evaluation_strategy", "steps",
"--overwrite_cache", 
"--overwrite_output_dir", 
"--model_name_or_path", "roberta-base", 
"--task_name", "sst2", 
"--do_train", 
"--do_eval", 
"--max_seq_length", "512", 
"--per_device_train_batch_size", "8",
// "--gradient_accumulation_steps", "2",
"--per_device_eval_batch_size", "64",
"--learning_rate", "2e-5",
"--weight_decay", "0.01",
"--lr_scheduler_type", "cosine",
"--num_train_epochs", "5", 
"--output_dir", "/tmp/conv-roberta/", 
"--validation_file", "/home/ubuntu/data/convmatch/205_val.csv", 
"--train_file", "/home/ubuntu/data/convmatch/205_train.csv"
]