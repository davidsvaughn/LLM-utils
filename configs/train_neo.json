["llm_train.py", 
"--eval_steps", "100",
"--save_steps", "100",
"--logging_steps", "201",
"--evaluation_strategy", "steps",
"--group_by_length", "True", "--pad_to_max_length", "False", "--fp16", 
"--overwrite_cache", 
"--overwrite_output_dir", 
"--model_name_or_path", "EleutherAI/gpt-neo-125M", 
"--tokenizer_name", "gpt2", 
"--use_fast_tokenizer", "False", 
"--task_name", "sst2", 
"--do_train", 
"--do_eval", 
"--max_seq_length", "512", 
"--per_device_train_batch_size", "16",
"--gradient_accumulation_steps", "2",
// "--per_device_train_batch_size", "4",
// "--gradient_accumulation_steps", "4",
"--per_device_eval_batch_size", "64",
"--learning_rate", "5e-5",
"--weight_decay", "0.01",
// "--lr_scheduler_type", "cosine",
"--num_train_epochs", "3",
"--output_dir", "/tmp/conv-34-neo/", 
"--validation_file", "/home/david/code/davidsvaughn/conv/data/convmatch/grades/proc/34_val.csv", 
"--train_file", "/home/david/code/davidsvaughn/conv/data/convmatch/grades/proc/34_train.csv"
]