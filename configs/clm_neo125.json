["/home/david/code/davidsvaughn/conv/gptj/Finetune_GPTNEO_GPTJ6B/finetuning_repo/run_clm.py", "--local_rank=0", "--deepspeed", "/home/david/code/davidsvaughn/conv/gptj/Finetune_GPTNEO_GPTJ6B/finetuning_repo/ds_config_gptneo.json", "--model_name_or_path", "EleutherAI/gpt-neo-125M", "--train_file", "/home/david/code/davidsvaughn/conv/gptj/Finetune_GPTNEO_GPTJ6B/finetuning_repo/train.csv", "--validation_file", "/home/david/code/davidsvaughn/conv/gptj/Finetune_GPTNEO_GPTJ6B/finetuning_repo/validation.csv", "--do_train", "--do_eval", "--fp16", "--overwrite_cache", "--evaluation_strategy=steps", "--output_dir", "finetuned", "--num_train_epochs", "12", "--eval_steps", "8", "--gradient_accumulation_steps", "32", "--per_device_train_batch_size", "2", "--use_fast_tokenizer", "False", "--learning_rate", "5e-06", "--warmup_steps", "10", "--save_total_limit", "1", "--save_steps", "8", "--save_strategy", "steps", "--tokenizer_name", "gpt2", "--block_size", "1024"]