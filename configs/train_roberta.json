["llm_train.py",
"--eval_steps", "100",
"--save_steps", "100",
"--logging_steps", "201",
"--evaluation_strategy", "steps",
"--group_by_length", "True", "--pad_to_max_length", "False", "--fp16", 
// "--overwrite_cache", 
// "--overwrite_output_dir", 
"--model_name_or_path", "roberta-base", 
"--task_name", "sst2",
"--do_train", 
"--do_eval", 
"--max_seq_length", "512", 
"--per_device_train_batch_size", "16",
"--gradient_accumulation_steps", "2",
"--per_device_eval_batch_size", "64",
"--learning_rate", "2e-5",
"--weight_decay", "0.01",
"--lr_scheduler_type", "cosine",
"--num_train_epochs", "3", 
"--output_dir", "/tmp/conv-207-roberta/", 
"--validation_file", "/home/david/code/davidsvaughn/conv/data/convmatch/grades/proc/207_val.csv", 
"--train_file", "/home/david/code/davidsvaughn/conv/data/convmatch/grades/proc/207_train.csv"
]