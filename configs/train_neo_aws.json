["llm_train.py", 
// "--model_name_or_path", "EleutherAI/gpt-neo-125M", "--deepspeed", "ds_config_gptneo.json",
// "--model_name_or_path", "EleutherAI/gpt-neo-1.3B", "--deepspeed", "ds_config_gptneo.json",
// "--model_name_or_path", "EleutherAI/gpt-neo-2.7B", "--deepspeed", "ds_config_gptneo.json",
"--model_name_or_path", "EleutherAI/gpt-j-6B", "--deepspeed", "ds_config_gptj6b.json",
"--warmup_steps", "10",
"--tokenizer_name", "gpt2", 
"--save_steps", "10000",
"--logging_steps", "201",
"--eval_steps", "25",
"--evaluation_strategy", "steps",
// "--overwrite_cache", 
"--overwrite_output_dir", 
"--use_fast_tokenizer", "False", 
"--task_name", "sst2", 
"--do_train", 
"--do_eval", 
// "--fp16", 
"--max_seq_length", "512", 
"--per_device_train_batch_size", "1",
"--gradient_accumulation_steps", "4",
// "--per_device_train_batch_size", "4",
// "--gradient_accumulation_steps", "4",
"--per_device_eval_batch_size", "8",
"--learning_rate", "2e-5",
"--weight_decay", "0.01",
// "--lr_scheduler_type", "cosine",
"--num_train_epochs", "2",
"--output_dir", "/tmp/conv-neo/", 
"--validation_file", "/home/ubuntu/data/convmatch/205_val.csv", 
"--train_file", "/home/ubuntu/data/convmatch/205_train.csv"
]