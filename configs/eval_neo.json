// This does evaluation using the training script, by just cutting off training after 10 batches...
["llm_train.py", 
"--eval_steps", "10",
"--save_steps", "1000",
"--logging_steps", "201",
"--evaluation_strategy", "steps",
"--group_by_length", "True", "--pad_to_max_length", "False", "--fp16", 
// "--overwrite_cache", 
"--overwrite_output_dir", 
"--tokenizer_name", "gpt2", 
"--use_fast_tokenizer", "False", 
"--task_name", "sst2", 
"--do_train", 
"--do_eval", "--ignore_mismatched_sizes",
"--max_seq_length", "512", 
"--per_device_train_batch_size", "16",
"--gradient_accumulation_steps", "2",
"--per_device_eval_batch_size", "64",
"--learning_rate", "5e-10",
"--weight_decay", "0.01",
"--num_train_epochs", "3",
"--output_dir", "/tmp/tmp/",
//////
"--model_name_or_path", "./models/conv/203/neo-836",
"--train_file", "/home/david/code/davidsvaughn/conv/data/convmatch/grades/proc/203_train.csv",
"--validation_file", "/home/david/code/davidsvaughn/conv/data/convdata/items/proc/101608.csv"
//////
]